{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math for Machine Learning\n",
    "\n",
    "AWS MACHINE LEARNING: MATH FOR MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 1 - Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "* Logistic regression - transform linear regression by a sigmoid activation function\n",
    "* Often we regularization with early stopping to counteract overfitting\n",
    "* Early stopping is an approximate equivalent of L2 regularization, often used in its place because it's computationally cheaper\n",
    "* In practice, we use L1, L2 and early stopping\n",
    "* Use a threshold, e.g., 50% or higher is a yes\n",
    "* TP, TN, FP, FN\n",
    "* Precision - TP / all positives\n",
    "* Recall / sensitivity / TP rate - TP / (TP + FN) (or anything you **predicted** was true)\n",
    "* Tune threshold to optimize metric of your choice\n",
    "* Use the ROC curve to choose the decision threshold based on decision criteria\n",
    "\t* ROC is built by looking at FP rate (x-axis) by TP rate (y-axis) for all possible threshold choices\n",
    "* Use AUC (area under curve) as an aggregate measure of performance across all possible classification thresholds\n",
    "* AUC helps you choose between models when you don't know what decision threshold is ultimately going to be used\n",
    "* It's like asking, \"If we pick a random positive (1) and random negative (0) case, what's the probability my model will score them in the correct relative order?\"\n",
    "* AUC is scale-invariant and classification threshold-invariant\n",
    "* People also use something like this for a precision / recall curve (or precision / recall gain curve) with different metrics on the axes (here, precision and recall, duh)\n",
    "* Logistic regression predictions should be unbiased, that is, average of predictions == average of observations\n",
    "* No bias doesn't necessarily mean no problem, but having bias means you **do** have a problem.  Examples of what can cause bias:\n",
    "    * Incomplete feature set\n",
    "    * Buggy pipeline\n",
    "    * Biased training sample\n",
    "* You can look for bias in slices of your data - take a look at a calibration plot - plots fitted values versus the actual average values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid: $$\\hat{y} = \\frac{1}{1 + e^{-(w^T x + b)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $$w^T x + b$$ is the input into the sigmoid (normally the output of the linear model).  We're squishing through a sigmoid function with raising `e` to the negative power of that value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of logistic regression is a calibrated probability estimate.  This is useful because we can cast binary classification problems into probabilistic problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is the cumulative distribution function of the logistic probability distribution whose quantile function is the inverse of the logit which models the log odds.  Therefore, mathematically, the opposite of a sigmoid can be considered probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, use cross entropy (which is the log loss) as the error metric.  Related to Shannon's information theory.  Loss function we use for backprop.\n",
    "\n",
    "Less emphasis on errors where output is relatively close to the label.\n",
    "\n",
    "Cross entropy grows exponentially when the prediction is close to the opposite label.  Very high penalty for getting something wrong, and being very confident about it.\n",
    "\n",
    "Derivative of MSE could cause problems with training due to vanishing gradients.\n",
    "\n",
    "$$LogLoss = \\sum_{(\\textbf{x}, y) \\in D} -ylog (\\hat{y}) - (1 - y)log(1 - \\hat{y})$$\n",
    "\n",
    "Weights will be driven to$$- \\infty$$ or $$+ \\infty$$ if we train long enough\n",
    "\n",
    "Near asymptotes, sigmoid function becomes flatter and flatter, so gradients start to disappear (derivative gets closer and closer to zero).  Since we use derivative in backprop, training will stop if gradient actually becomes zero.\n",
    "\n",
    "When this happens to all inputs, it's called **Saturation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding regularization to logistic regression helps keep the model simpler by having smaller parameter weights.  This penalty term added to the loss function makes sure that cross-entropy through gradient descent doesn't keep pushing the weights closer and closer to +/- infinity and causing numerical issues.  Also, we can stay closer to the middle of the sigmoid, allowing weights to update and tuning to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Neural Networks\n",
    "\n",
    "* NN's combine features as an alternative to feature crosses\n",
    "* Structure model so features are combined, then combinations may be combined with another layer, repeat\n",
    "* Get the model to learn these through training\n",
    "* If you just use linear transformation functions in the neurons of your NN layers, you just get a linear function, no learning\n",
    "* Instead, use a non-linear transformation layer, aka an activation function\n",
    "* Usually, all layers except last in NN are non-linear activations, and last layer is linear for regression or softmax or sigmoid for classification\n",
    "* ReLu activations are great but can still lead to vanishing gradients - check out softplus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Softplus = ln(1 + e^{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, ln = natural log = \n",
    "\n",
    "$$\n",
    "log_e\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, num=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00671535, 0.00742646, 0.00821257, 0.00908152, 0.01004194,\n",
       "       0.01110337, 0.01227631, 0.01357232, 0.01500412, 0.01658572,\n",
       "       0.01833251, 0.02026142, 0.02239102, 0.0247417 , 0.02733581,\n",
       "       0.03019782, 0.03335452, 0.03683516, 0.04067169, 0.04489892,\n",
       "       0.04955473, 0.05468026, 0.06032013, 0.06652261, 0.07333982,\n",
       "       0.08082786, 0.08904698, 0.09806167, 0.10794073, 0.11875731,\n",
       "       0.1305888 , 0.14351678, 0.15762678, 0.17300799, 0.18975286,\n",
       "       0.20795657, 0.22771641, 0.24913098, 0.27229932, 0.2973199 ,\n",
       "       0.32428949, 0.3533019 , 0.38444679, 0.41780824, 0.45346348,\n",
       "       0.49148158, 0.53192222, 0.57483459, 0.62025647, 0.66821347,\n",
       "       0.71871852, 0.77177162, 0.82735984, 0.88545757, 0.94602703,\n",
       "       1.00901904, 1.07437389, 1.14202255, 1.21188776, 1.28388545,\n",
       "       1.35792597, 1.43391548, 1.51175724, 1.59135277, 1.67260304,\n",
       "       1.75540942, 1.83967466, 1.92530355, 2.01220365, 2.10028577,\n",
       "       2.18946438, 2.27965791, 2.37078894, 2.46278435, 2.55557533,\n",
       "       2.64909739, 2.74329029, 2.83809791, 2.93346813, 3.02935271,\n",
       "       3.125707  , 3.22248987, 3.31966344, 3.4171929 , 3.51504631,\n",
       "       3.6131944 , 3.71161039, 3.81026981, 3.90915031, 4.0082315 ,\n",
       "       4.10749481, 4.20692331, 4.30650161, 4.4062157 , 4.50605287,\n",
       "       4.60600154, 4.70605122, 4.80619237, 4.90641636, 5.00671535])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1 + math.e**x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d74cf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, np.log(1 + math.e**x), 'g-')\n",
    "plt.title('Softplus')\n",
    "plt.axis([-5, 5, -5, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The derivative of the Softplus function is the logistic function\n",
    "* Pros of Softplus:  Continuous, differentiable at zero\n",
    "* Cons of Softplus:  Due to log and exponential, there's added computation compared to ReLUs, and ReLUs still have as good performance in practice\n",
    "* Therefore, Softplus is usually discouraged to be used in deep learning\n",
    "\n",
    "* Leaky ReLU - same as ReLU, but in negative domain, gradient is 0.01\n",
    "* Parametric ReLU (PReLU) - instead of having gradient of 0.01 in negative domain, it's $$\\alpha(x)$$\n",
    "\n",
    "* Alpha is a learned parameter, updated during training\n",
    "* Randomized Leaky ReLU - Instead of alpha being trained, it's sampled from a uniform distribution randomly\n",
    "    * Can have an effect similar to dropout since we technically have a different network for each value of alpha, so it's something like an ensemble\n",
    "    * At test time, all values of alpha are averaged together\n",
    "* ReLU6 - still zero in negative domain, but in positive domain, it's capped at 6\n",
    "* Adding layers to a NN allows it to learn more complex functions\n",
    "* Adding nodes to a layer adds new dimensions to vector space\n",
    "* Neural networks can be arbitrarily complex. To increase hidden dimensions, I can add neurons. To increase function composition, I can add layers. If I have multiple labels per example, I can add outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sqrt{x^2}$$\n",
    "\n",
    "$$\\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 2 - Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\vec{v} \\cdot \\vec{w} = \\vec{v}^T \\cdot \\vec{w} = \\sum_{i = 1}^{n}v_i w_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product allows us to extend the notion of angle to **all dimensions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dot product](images/dotproduct001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec{v} = \\begin{pmatrix}v_1\\\\0\\end{pmatrix} \\vec{w} = \\begin{pmatrix}w_1\\\\w_2\\end{pmatrix}$\n",
    "\n",
    "$\\vec{v} \\cdot \\vec{w} = v_1 \\cdot w_1 + 0 \\cdot w_2 = v_1 \\cdot w_1$\n",
    "\n",
    "$= v_1 \\left\\lVert \\vec{w} \\right\\rVert \\cos \\theta$\n",
    "\n",
    "$= \\left\\lVert \\vec{v} \\right\\rVert \\cdot \\left\\lVert \\vec{w} \\right\\rVert \\cos \\theta$\n",
    "\n",
    "$\\vec{v} \\cdot \\vec{w} = \\left\\lVert \\vec{v} \\right\\rVert \\cdot \\left\\lVert \\vec{w} \\right\\rVert \\cos \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product is the same thing as the product of the norms multiplied by cosine of the angle between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta = \\arccos \\frac{\\vec{v} \\cdot \\vec{w}}{\\left\\lVert \\vec{v} \\right\\rVert \\cdot \\left\\lVert \\vec{w} \\right\\rVert}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orthogonality: $\\vec{v} \\cdot \\vec{w} = 0 $ \n",
    "\n",
    "Assume $\\left\\lVert \\vec{v} \\right\\rVert , \\left\\lVert \\vec{w} \\right\\rVert \\neq 0$ (that is, no zero-length vectors)\n",
    "\n",
    "Only when $cos \\theta$ is 0 can $\\vec{v} \\cdot \\vec{w} $ be zero, so $\\theta$ = either $\\frac{-\\pi}{2}$ or $\\frac{\\pi}{2}$ (i.e., -90 degrees or 90 degrees)\n",
    "\n",
    "All this means is that we're talking about two vectors at right angles to one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the $L_1$ norm of a vector is just the sum of the absolute values of the elements in the vector, and the $L_2$ norm (aka Euclidian norm) is the square root of the sum of the squares of the elements in the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\cos \\theta$ is only 0 at $\\frac{-\\pi}{2}$ and $\\frac{\\pi}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case where $\\vec{v} \\cdot \\vec{w} > 0 $ \n",
    "\n",
    "$= \\left\\lVert \\vec{v} \\right\\rVert \\cdot \\left\\lVert \\vec{w} \\right\\rVert \\cos \\theta$\n",
    "\n",
    "Which means that $\\cos \\theta$ > 0\n",
    "\n",
    "So where is this true?  It's where $\\cos \\theta$ lies between $\\frac{-\\pi}{2}$ or $\\frac{\\pi}{2}$ or some angle that's < 90 degrees compared to v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dot product](images/dotproduct002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, this works the same no matter how many dimensions we're talking about.\n",
    "\n",
    "#### Negative Inner Product\n",
    "\n",
    "$ = \\left\\lVert \\vec{v} \\right\\rVert \\cdot \\left\\lVert \\vec{w} \\right\\rVert < 0 $\n",
    "\n",
    "$ = \\left\\lVert \\vec{v} \\right\\rVert \\cdot \\left\\lVert \\vec{w} \\right\\rVert \\cos \\theta < 0$\n",
    "\n",
    "$ = \\cos \\theta < 0 $\n",
    "\n",
    "$ | \\theta | > \\frac{\\pi}{2} $\n",
    "\n",
    "This means angle is > 90 degrees\n",
    "\n",
    "![dot product](images/dotproduct003.png)\n",
    "\n",
    "To summarize:\n",
    "\n",
    "* If the dot product is positive, the vectors are pointing somewhat in the same direction\n",
    "* If the dot product is zero, the vectors are orthogonal to one another\n",
    "* If the dot product is negative, the vectors are pointing somewhat away from each other\n",
    "\n",
    "#### Hyperplane Definition\n",
    "\n",
    "* It is the thing orthogonal to a given vector\n",
    "* All hyperplanes pass through the zero point or origin for your vector (0,0 for 2-dimensions, 0,0,0 for 3-dimensions, etc.) OR a translated to a different point\n",
    "* Geometric notion of a hyperplane is just some subspace of your given high dimensional space that separates it into two equal parts\n",
    "\n",
    "#### Decision Plane\n",
    "\n",
    "* Separates whether or not a feature passes a certain threshold\n",
    "\n",
    "Given a weight vector $ \\left\\lVert \\vec{w} \\right\\rVert $ and a data vector $ \\left\\lVert \\vec{v} \\right\\rVert $ we're asking if this weight vector applied to the data vector exceeds some threshold C\n",
    "\n",
    "Think of $ \\vec{w} $ as being fixed, we're looking to see if $ C < \\left\\lVert \\vec{w} \\right\\rVert \\cdot \\left\\lVert \\vec{v} \\right\\rVert \\cdot \\cos \\theta $ \n",
    "\n",
    "Dividing by $ \\vec{w} $ we get:\n",
    "\n",
    "$ \\left\\lVert \\vec{v} \\right\\rVert \\cos \\theta > \\frac{C}{\\left\\lVert \\vec{w} \\right\\rVert} $ \n",
    "\n",
    "When we say, \"Does w dot v exceed C\" it's the same thing as \"Does norm v times cosine of angle between them exceed this C over norm w\"\n",
    "\n",
    "Norm v * cosine theta is length of the orthogonal projection of $ \\vec{v} $ onto $ \\vec{w} $\n",
    "\n",
    "So what we're asking is if the length of the projection is longer than $ \\frac{C}{\\left\\lVert \\vec{w} \\right\\rVert} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dot product](images/dotproduct004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orange line represents $ \\left\\lVert \\vec{w} \\right\\rVert \\cdot \\left\\lVert \\vec{v} \\right\\rVert = C $\n",
    "\n",
    "![dot product](images/dotproduct005.png)\n",
    "\n",
    "So this is the decision plane that separates one side from the other, in this case, to the upper right, $ \\left\\lVert \\vec{w} \\right\\rVert \\cdot \\left\\lVert \\vec{v} \\right\\rVert > C $ and if you're to the lower left, $ \\left\\lVert \\vec{w} \\right\\rVert \\cdot \\left\\lVert \\vec{v} \\right\\rVert < C $\n",
    "\n",
    "Think of this as the decision of whether an image is classified as a cat or a dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "\n",
    "If A is a matrix where the rows are features $ w_i $ and B is a matrix where the columns are data vectors $ v_j $ then the i,j-th entry of the product is $ w_i v_j $, which is to say the i-th feature of the j-th vector.\n",
    "\n",
    "In formulae:  if C = AB, where A is an n x m matrix and B is an m x k matrix, then C is an n x k matrix where\n",
    "\n",
    "$$c_{i , j} = \\sum_{l} a_{i,l} b_{l,j}$$\n",
    "\n",
    "Which just means \"dot product of i-th row by j-th column\"\n",
    "\n",
    "Can chain together many matrix multiplications as long as the \"inner dimensions\" are the same for each pair\n",
    "\n",
    "$ A_1 A_2 A_3 A_4 $\n",
    "\n",
    "This works if $ A_1 $ is an $ n_1 $ by $ n_2 $ matrix and $ A_2 $ is an $ n_2 $ by $ n_3 $ matrix, $ A_3 $ is an $ n_3 $ by $ n_4 $ matrix, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hadamard Product\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An often less useful method of multiplying matrices is **element-wise**\n",
    "\n",
    "Two matrices A and B that have the same shape (e.g., 3x2 and 3x2) - just multiply elements in same position, results in same shape matrix (here, 3x2)\n",
    "\n",
    "Normally denoted with a circle, like this:  $ A \\circ B $\n",
    "\n",
    "This does come up when you're dealing with derivatives of matrix-valued functions, which you see oftentimes in ML applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Properties\n",
    "\n",
    "Remember that matrix multiplication is commutative if - and only when - both of the matrices are diagonal and of equal dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometry Matrix Operation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invertibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 3 - Probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 4 - Univariate Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 5 - Multivariate Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
