{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Model Productionizing\n",
    "\n",
    "In this section of the course, we'll focus on the production phase of the machine learning pipeline. We'll review the cycle of machine learning projects, and examine how AWS services can help the storage, monitoring, and maintenance aspects of model production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ML Models in Production\n",
    "\n",
    "* Integrating an ML solution with existing software\n",
    "* Keeping it running successfully over time\n",
    "\n",
    "#### Aspects to consider\n",
    "\n",
    "* Model hosting\n",
    "* Model deployment\n",
    "* Pipelines to provide feature vectors\n",
    "* Code to provide low-latency and/or high-volume predictions\n",
    "* Model and data updating and versioning\n",
    "* Quality monitoring and alarming\n",
    "* Data and model security and encryption\n",
    "* Customer privacy, fairness, and trust\n",
    "* Data provider contractual constraints (e.g., attribution, cross-fertilization)\n",
    "\n",
    "#### Types of production environments\n",
    "\n",
    "##### Batch predictions\n",
    "* Useful if all possible inputs known a priori (e.g., all product categories for which demand is to be forecast, all keywords to bid)\n",
    "* Predictions can still be served real-time, simply read from pre-computed values\n",
    "\n",
    "##### Online predictions\n",
    "* Useful if input space is large (e.g., customer's utterances or photos, detail pages to be translated)\n",
    "* Low latency requirement (e.g., at most 100ms)\n",
    "\n",
    "##### Online training\n",
    "* Sometimes training data patterns change often, so need to train online (e.g., fraud detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Metrics\n",
    "\n",
    "* Business metrics may not be the same as the performance metrics that are optimized during training.  Why?\n",
    "    * Example - click-through rate\n",
    "* Ideally, performance metrics are highly correlated with business metrics\n",
    "* Confusion matrix, TP, FP, TN, FN, precision, recall, etc.\n",
    "* Issue: In many applications, TN dwarfs the other categories, making accuracy useless for comparing models\n",
    "* F1-score - combines precision and recall - it's the harmonic mean of precision and recall\n",
    "\n",
    "$$ F_1 Score = \\frac{2 \\cdot precision \\cdot recall}{precision + recall} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "* Issue - metrics on training data can't measure generalization\n",
    "    * Model could cheat by memorizing the data and getting a perfect scoe\n",
    "    * Overfitting\n",
    "* Solution - Cross-validation: Train and evaluate on distinct data sets\n",
    "* `from sklearn.model_selection import train_test_split`\n",
    "* Overfitting?  Reduce the number of predictor variables\n",
    "* `from sklearn.metrics import precision_score, recall_score, f1_score`\n",
    "* For cancer, recall is most important to make sure we don't miss those who have cancer (but I'd say it's also super important to not treat someone for cancer who doesn't have it!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation\n",
    "\n",
    "* Issue: small sets\n",
    "    * Smaller training set -> not enough data for good training\n",
    "    * Unrepresentative test set -> invalid metrics\n",
    "* K-fold cross-validation\n",
    "    * Randomly partition data into K \"folds\" (larger K means more time and more variance)\n",
    "    * For each fold, train model on other K-1 folds and evaluate on that\n",
    "    * Train on all data\n",
    "    * Average metric across K folds estimates test metric for trained model\n",
    "* Choosing K\n",
    "    * Large -> more time, more variance\n",
    "    * Small -> more bias\n",
    "    * 5-10 is typical\n",
    "* Leave-one-out cross-validation\n",
    "    * K = n\n",
    "    * Use for very small datasets\n",
    "* Stratified K-fold cross-validation\n",
    "    * Preserve class proportions in the folds\n",
    "    * Use for imbalanced data\n",
    "    * There are seasonality or subgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Linear Regression\n",
    "\n",
    "* MSE - average squared error over entire dataset - very commonly used - `sklearn.metrics.mean_squared_error`\n",
    "* $ R^2 $\n",
    "    * $ R^2 = 1 - \\frac{MSE}{Var(y)} $ which is between 0 and 1\n",
    "    * Interpretation: Fraction of variance accounted for by the model\n",
    "    * Basically, standardized version of MSE\n",
    "    * Good $ R^2 $ are determined by actual problem\n",
    "    * $ R^2 $ always increases when more variables are added to the model (can lead to overfitting - highest $ R^2 $ may not be the best model)\n",
    "    * Adjusted $ R^2 $: Take into account of the effect of adding more variables such that it only increases when the added variables have significant effect in prediction\n",
    "        * Better metric for multiple variates regression (multivariate regression?)\n",
    "    * `sklearn.metrics.r2_score`\n",
    "* Gaussian - probability density function\n",
    "\n",
    "$$ f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "![Gaussian](images/gaussian001.png)\n",
    "\n",
    "* Why do we study normal distribution so often?\n",
    "* Central Limit Theorem - no matter what is the original distrigution of X, the mean of X (i.e. \\bar{X}) will follow a normal distribution:\n",
    "\n",
    "$$ \\bar{X} ~ N (\\mu, \\frac{\\sigma^2}{n} )$$\n",
    "\n",
    "#### Confidence interval\n",
    "* An average computed on a sample is merely an **estimate** of the true population mean\n",
    "* Confidence interval: Quantifies margin-of-error between sample metric and true metric due to **sampling randomness**\n",
    "* Informal interpretation: With x% confidence, true metric lies within the interval\n",
    "* Precisely: If the true distribution is as stated, then with x% probability the observed value is in the interval\n",
    "    * Differently stated for a 90% confidence interval:  If you randomly draw data from the distribution 100 times and make 100 predictions, of those 100 predictions, 90 will fall into the confidence interval you reported\n",
    "* For population proportion (i.e. the truth), the confidence interval is:\n",
    "\n",
    "$$ CI = p \\pm z(p(1-p)/n)^{1/2} $$\n",
    "\n",
    "* Where p is the sample proportion, n is sample size, and z is z-score defined for that confidence level (e.g., 90, 95, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ML Models in Production: Storage\n",
    "\n",
    "#### Data Formats\n",
    "* Row-oriented formats\n",
    "    * Comma/tab-separated values (CSV/TSV)\n",
    "    * Read-only DB (RODB): Internal read-only file-based store with fast key-based access\n",
    "    * Avro - allows schema evaluation for Hadoop\n",
    "* Column-oriented formats\n",
    "    * Parquet: Type-aware and indexed for Hadoop\n",
    "    * Optimized row columnar (ORC): Type-aware, indexed, and with statistics for Hadoop\n",
    "* User-defined formats\n",
    "    * JSON: For key-value objects\n",
    "    * Hierarchical data format 5 (HDF5): Flexible data model with chunks\n",
    "* Compression can bre applied to all formats\n",
    "* Usual trade-offs: Read/write speeds, size, platform-dependency, ability for schema to evolve, schema/data separability, type richness\n",
    "\n",
    "#### Model and Pipeline Persistence\n",
    "* Predictive Model Markup Language (PMML)\n",
    "    * Vendor-independent XML-based language for storing ML models\n",
    "    * Support varies in different libraries\n",
    "        * KNIME (analytics/ML library): Full support\n",
    "        * Scikit-learn: Extensive support\n",
    "        * Spark MLlib: Limited support\n",
    "* Custom methods\n",
    "    * Scikit-learn: Uses the Python pickle method to serialize/deserialize Python objects\n",
    "    * Spark MLlib: Transformers and Estimators implement MLWritable\n",
    "    * TensorFlow: Allows saving of MetaGraph\n",
    "    * MxNet: Saves into JSON\n",
    "    \n",
    "#### Model Deployment\n",
    "* Technology transfer: Experimental framework may not suffice for production\n",
    "    * A/B testing or shadow testing: helps catch production issues early\n",
    "    * [Rules of Machine Learning: Best Practices for ML Engineering (2017)](http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf)\n",
    "    \n",
    "#### Information Security\n",
    "* Make sure that you handle training and evaluation data in accordance with data classification\n",
    "* Models may need to be treated with same classification level as source data.  Why?  Model parameters come from training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ML Models in Production: Monitoring and Maintenance\n",
    "\n",
    "* It's important to monitor quality metrics and business impacts with dashboards, alarms, user feedback, etc.\n",
    "    * The real-world domain may change over time (\"model drift\")\n",
    "    * Software environment may change\n",
    "    * High profile special cases may fail\n",
    "    * There may be a change in business goals\n",
    "* Performance deterioration may require new tuning\n",
    "    * Changing goals may require new metrics\n",
    "    * A changing domain may require changes to validation set\n",
    "    * Your validation set may be replaced over time to avoid overfitting\n",
    "    * Features may no longer be available\n",
    "* Customer obsession\n",
    "    * Think carefully about the impact on customer perception and trust\n",
    "    * Give ML solutions the \"creepiness sniff test\" and \"The front page of a newspaper test\"\n",
    "    * Provide explanations to customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ML Models in Production: Using AWS\n",
    "\n",
    "* SageMaker - build, train, and deploy machine learning models at scale\n",
    "    * built-in, high performance algorithms\n",
    "    * one-click training\n",
    "    * hyperparameter optimization\n",
    "    * one-click deployment\n",
    "    * fully managed hosting w/ auto scaling\n",
    "* Rekognition - images and video, billions of images daily\n",
    "* Lex - chatbot ASR and NLU\n",
    "* Polly - text-to-voice - more than two dozen languages\n",
    "* Comprehend - entity recognition, topic modeling, sentiment analysis, etc.\n",
    "* Translate - neural machine translation service\n",
    "* Transcribe - speech to text\n",
    "* DeepLens\n",
    "* AWS Glue - data integration service for managing ETL jobs\n",
    "* [Deep Scalable Sparse Tensor Network Engine (pronounced \"Destiny\")](https://github.com/amzn/amazon-dsstne) - Neural network engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Mistakes\n",
    "\n",
    "* You solved the wrong problem \n",
    "    * Interactions b/t data science and business teams must be early and often\n",
    "* The data was flawed\n",
    "    * Big data $ \\neq $ good data\n",
    "* The solution didn't scale\n",
    "* Final result doesn't match with the prototype's results\n",
    "* It takes too long to fail\n",
    "    * Pull the plug if there's a strong indication the project isn't going to work - the sooner you stop a failing project the sooner you can start a successful project\n",
    "* The solution was too complicated\n",
    "* There weren't enough allocated engineering resources to try out long-term science ideas\n",
    "* There was a lack of collaboration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
