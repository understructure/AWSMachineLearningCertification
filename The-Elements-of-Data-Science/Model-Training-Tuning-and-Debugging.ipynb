{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Training, Tuning, and Debugging\n",
    "\n",
    "### Supervised Learning: Neural Networks\n",
    "\n",
    "* Simplest NN is a perceptron - single layer\n",
    "* Bias is like an intercept, have linear combination of features, pass through (e.g., sigmoid) activation function\n",
    "* Layers of nodes\n",
    "* each node is one multivariate linear function with aa univariate nonlinear transformation\n",
    "* Trained via (stochastic) gradient descent\n",
    "* Can represent any non-linear function (very expressive)\n",
    "* Generally hard to interpret\n",
    "* Expensive to train, fast to predict\n",
    "* scikit-learn: `sklearn.neural_network.MLPClassifier`\n",
    "* Deep learning frameworks:\n",
    "    * MXNet\n",
    "    * TensorFlow\n",
    "    * Caffe\n",
    "    * PyTorch\n",
    "* Convolutional NNs - input is image or sequence image\n",
    "* Use filters to create the next layer\n",
    "* Pooling layer - reduce the size w/ max or average pooling\n",
    "* Reduce the size of the data to aid convergence\n",
    "* Turn into fully connected layer(s) at end\n",
    "\n",
    "#### Recurrent Neural Network (RNN)\n",
    "\n",
    "* Works well with sequence or time-shared features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: K-Nearest Neighbors \n",
    "\n",
    "* Define a distance metric\n",
    "    * Euclidian\n",
    "    * Manhattan\n",
    "    * Any vector norm can be used as a measure of distance\n",
    "* Choose the number of K neighbors\n",
    "* Find the K nearest neighbors of the new observation that we want to classify\n",
    "* Assign class label by majority vote\n",
    "* Important to find the right K\n",
    "* Commonly use $ K = \\frac{\\sqrt{n}}{2} $ where n = number of samples\n",
    "    * K depends on your data\n",
    "* Smaller k = more local behavior, larger k = more global behavior\n",
    "* Non-parametric, instance-based, lazy\n",
    "    * Non parametric - model is not defined by fixed set of parameters\n",
    "    * Instance-based or lazy learning - Model is the result of effectively memorizing training data\n",
    "* Requires keeping the original data set - can be very expensive\n",
    "* Space complexity and prediction-time complexity grow with size of training data\n",
    "* Suffers from curse of dimensionality - points become increasingly isolated with more dimensions, for a fixed-size training dataset\n",
    "* scikit-learn: `sklearn.neighbors.KNeighborsClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: Linear and Non-Linear Support Vector Machines\n",
    "\n",
    "![SVM](images/svm.png)\n",
    "\n",
    "#### Linear SVM\n",
    "\n",
    "* What really matters is the points that lie on the margins, or the support vectors\n",
    "* Optimal hyperplane separates two classes\n",
    "* Very popular in research\n",
    "* Simplest case: maximixe the margin - the distance b/t the decision boundary (hyperplane) and the support vectors (training examples closest to boundary)\n",
    "* Max margin picture not applicable in non-separable case\n",
    "* scikit-learn: `sklearn.svm.SVC`\n",
    "\n",
    "#### Non-linear SVM\n",
    "\n",
    "* Also popular approach in research\n",
    "* \"Kernelize\" for nonlinear problems:\n",
    "    * Choose a distance function called a \"kernel\"\n",
    "    * Map the learning task to a higher-dimension space\n",
    "    * Apply a linear SVM classifier in the new space\n",
    "* Not memory-efficient, because it stores the support vectors, which grow with the size of the training data\n",
    "* Computation is expensive\n",
    "* scikit-learn: `sklearn.svm.SVC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: Decision Trees and Random Forests\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "* Algorithm decides what to use as splits at what layer\n",
    "* Entropy - relative measure of disorder in the data source\n",
    "\n",
    "$$ H(X) = - \\sum_{i=1}^N P(x_i)log(P(x_i)) $$\n",
    "\n",
    "* Try to get data \"pure\" - each leaf has only 0's or 1's\n",
    "* Entropy is low when all classes in a node are the same\n",
    "* Nodes are split based on the feature taht has the largest information gain (IG) between parent node and its split nodes\n",
    "* One metric to quantify IG is to compare entropy before and after splitting\n",
    "* In a binary case, entropy is 0 if all samples belong to the same class for a node (i.e., pure)\n",
    "* Entropy is 1 if samples contain both classes with equal proportion (i.e., 50% for each class, chaos)\n",
    "* The splitting procedure can go iteratively at each child node until the end-nodes (or leaves) are pure (i.e., there is only one class in each node)\n",
    "    * But the splitting procedure usually stops at certain criteria to prevent overfitting\n",
    "\n",
    "* In summary\n",
    "    * Train / build the tree by maximizing IG to choose splits (i.e. the impurity of split sets are lower)\n",
    "    * Easy to interpret (superficially)\n",
    "    * Expressive = flexible\n",
    "    * Less need for feature transformations\n",
    "    * Susceptible to overfitting\n",
    "    * Must \"prune\" the tree to reduce potential overfitting\n",
    "    * scikit-learn: `sklearn.tree.DecisionTreeClassifier`\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "* Ensemble methods - learn multiple models an dcombine results, usually via majority vote or averaging\n",
    "* Set of decision trees, each learned from a different randomly sampled subset with replacement\n",
    "* Features to split on for each tree, randomly selected subset from original features\n",
    "* Prediction: average output probabilities\n",
    "* Increases diversity through random selection of training dataset and subset of features for each tree\n",
    "* Reduces variance through averaging\n",
    "* Each tree typically does not need to be pruned\n",
    "* More expensive to train and run\n",
    "* scikit-learn: `sklearn.ensemble.RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "#### K-means clustering\n",
    "\n",
    "* Iteratively separates data into K clusters, minimizing sum of distances to center of closest cluster\n",
    "    * Step 1 - assign each instance to closest cluster\n",
    "    * Step 2 - recompute each center from assigned instances\n",
    "* Guaranteed to converge to local optimum\n",
    "* Suffers from curse of dimensionality\n",
    "* scikit-learn: `sklearn.cluster.kmeans`\n",
    "* User must determine or provide number of clusters (K)\n",
    "* Error is determined using sum of squared errors (SSE)\n",
    "\n",
    "$$ SSE = \\sum_{j=1}^n\\sum_{i=1}^m\\left\\lVert x_i - c_j \\right\\rVert^2_2 $$\n",
    "\n",
    "* Where $ c_j $ is the $ j^{th} $ cluster centroid and $ x $ is the number of samples belonging to the $ j^{th} $ cluster\n",
    "* Calculate this once cluster structure has been stabilized\n",
    "\n",
    "* Elbow method - use the elbow point as a starting point to determine how many clusters you should use\n",
    "    * More clusters implies smaller within-cluster SSE\n",
    "    * The decline of SSE (y-axis) slows down after the optimum number of clusters (x-axis) (i.e. the elbow point)\n",
    "\n",
    "![elbow](images/elbow.png)\n",
    "\n",
    "* Remember that if each cluster reaches size = 1, SSE will be zero but it's pretty useless\n",
    "\n",
    "#### Hierarchical clustering\n",
    "\n",
    "##### Agglomorative or \"Bottom-up\" \n",
    "* Bottom-up approach\n",
    "* Each data point begins as its own cluster\n",
    "\n",
    "##### Divisive\n",
    "* Top-down approach\n",
    "* Start with all points in a single cluster\n",
    "\n",
    "* Nested clusters with hierarchy\n",
    "* User doesn't need to provide number of clusters but needs to find a place to cut the dendrogram\n",
    "\n",
    "![elbow](images/dendrogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: Validation Set\n",
    "\n",
    "* Model training - improve model by optimizing parameters or data\n",
    "* Model tuning - tweak hyperparameters, looking for overfitting or underfitting\n",
    "* Motivation - model training and tuning involve comparing performance for different model or data settings\n",
    "* Problem - when you use the test set for these comparisons, that effectively makes it part of a training set (model may learn the patterns from the test set during training)\n",
    "* Solution - split training data into two parts - training and validation set\n",
    "    * Use training set to train candidate models, etc.\n",
    "    * The validation set plays the role of the test set during debugging and tuning\n",
    "    * Save the test set for measuring generalization of your final model\n",
    "* Validation set\n",
    "    * Issue - Splitting the training data into training and validation sets may make it too small or unrepresentative\n",
    "    * Solution - Use the holdout method to get the test set, then use k-fold cross validation on the training set for debugging and tuning\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: Bias Variance Tradeoff\n",
    "\n",
    "![bias-variance tradeoff](images/biasvar1.png)\n",
    "\n",
    "#### Using learning curves to evaluate the model\n",
    "\n",
    "* Motivation - detect if model is under- or overfitting, and impact of training data size the error\n",
    "* learning curves - plot training dataset and validation dataset error or accuracy against training set size\n",
    "* scikit-learn: `sklearn.learning_curve.learning_curve`\n",
    "    * Uses stratified k-fold cross-validation by default if output is binary or multiclass (preserves percentage of samples in each class)\n",
    "    * Note: `sklearn.model_selection.learning_curve` in v 0.18\n",
    "\n",
    "![bias-variance tradeoff](images/biasvar2.png)\n",
    "\n",
    "#### Learning Curves\n",
    "\n",
    "![learning curves](images/learningcurves.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Debugging: Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning: Bagging/Boosting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
