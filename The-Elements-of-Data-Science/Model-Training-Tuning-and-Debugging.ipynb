{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Training, Tuning, and Debugging\n",
    "\n",
    "### Supervised Learning: Neural Networks\n",
    "\n",
    "* Simplest NN is a perceptron - single layer\n",
    "* Bias is like an intercept, have linear combination of features, pass through (e.g., sigmoid) activation function\n",
    "* Layers of nodes\n",
    "* each node is one multivariate linear function with aa univariate nonlinear transformation\n",
    "* Trained via (stochastic) gradient descent\n",
    "* Can represent any non-linear function (very expressive)\n",
    "* Generally hard to interpret\n",
    "* Expensive to train, fast to predict\n",
    "* scikit-learn: `sklearn.neural_network.MLPClassifier`\n",
    "* Deep learning frameworks:\n",
    "    * MXNet\n",
    "    * TensorFlow\n",
    "    * Caffe\n",
    "    * PyTorch\n",
    "* Convolutional NNs - input is image or sequence image\n",
    "* Use filters to create the next layer\n",
    "* Pooling layer - reduce the size w/ max or average pooling\n",
    "* Reduce the size of the data to aid convergence\n",
    "* Turn into fully connected layer(s) at end\n",
    "\n",
    "#### Recurrent Neural Network (RNN)\n",
    "\n",
    "* Works well with sequence or time-shared features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: K-Nearest Neighbors \n",
    "\n",
    "* Define a distance metric\n",
    "    * Euclidian\n",
    "    * Manhattan\n",
    "    * Any vector norm can be used as a measure of distance\n",
    "* Choose the number of K neighbors\n",
    "* Find the K nearest neighbors of the new observation that we want to classify\n",
    "* Assign class label by majority vote\n",
    "* Important to find the right K\n",
    "* Commonly use $ K = \\frac{\\sqrt{n}}{2} $ where n = number of samples\n",
    "    * K depends on your data\n",
    "* Smaller k = more local behavior, larger k = more global behavior\n",
    "* Non-parametric, instance-based, lazy\n",
    "    * Non parametric - model is not defined by fixed set of parameters\n",
    "    * Instance-based or lazy learning - Model is the result of effectively memorizing training data\n",
    "* Requires keeping the original data set - can be very expensive\n",
    "* Space complexity and prediction-time complexity grow with size of training data\n",
    "* Suffers from curse of dimensionality - points become increasingly isolated with more dimensions, for a fixed-size training dataset\n",
    "* scikit-learn: `sklearn.neighbors.KNeighborsClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: Linear and Non-Linear Support Vector Machines\n",
    "\n",
    "![SVM](images/svm.png)\n",
    "\n",
    "#### Linear SVM\n",
    "\n",
    "* What really matters is the points that lie on the margins, or the support vectors\n",
    "* Optimal hyperplane separates two classes\n",
    "* Very popular in research\n",
    "* Simplest case: maximixe the margin - the distance b/t the decision boundary (hyperplane) and the support vectors (training examples closest to boundary)\n",
    "* Max margin picture not applicable in non-separable case\n",
    "* scikit-learn: `sklearn.svm.SVC`\n",
    "\n",
    "#### Non-linear SVM\n",
    "\n",
    "* Also popular approach in research\n",
    "* \"Kernelize\" for nonlinear problems:\n",
    "    * Choose a distance function called a \"kernel\"\n",
    "    * Map the learning task to a higher-dimension space\n",
    "    * Apply a linear SVM classifier in the new space\n",
    "* Not memory-efficient, because it stores the support vectors, which grow with the size of the training data\n",
    "* Computation is expensive\n",
    "* scikit-learn: `sklearn.svm.SVC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning: Decision Trees and Random Forests\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "* Algorithm decides what to use as splits at what layer\n",
    "* Entropy - relative measure of disorder in the data source\n",
    "\n",
    "$$ H(X) = - \\sum_{i=1}^N P(x_i)log(P(x_i)) $$\n",
    "\n",
    "* Try to get data \"pure\" - each leaf has only 0's or 1's\n",
    "* Entropy is low when all classes in a node are the same\n",
    "* Nodes are split based on the feature taht has the largest information gain (IG) between parent node and its split nodes\n",
    "* One metric to quantify IG is to compare entropy before and after splitting\n",
    "* In a binary case, entropy is 0 if all samples belong to the same class for a node (i.e., pure)\n",
    "* Entropy is 1 if samples contain both classes with equal proportion (i.e., 50% for each class, chaos)\n",
    "* The splitting procedure can go iteratively at each child node until the end-nodes (or leaves) are pure (i.e., there is only one class in each node)\n",
    "    * But the splitting procedure usually stops at certain criteria to prevent overfitting\n",
    "\n",
    "* In summary\n",
    "    * Train / build the tree by maximizing IG to choose splits (i.e. the impurity of split sets are lower)\n",
    "    * Easy to interpret (superficially)\n",
    "    * Expressive = flexible\n",
    "    * Less need for feature transformations\n",
    "    * Susceptible to overfitting\n",
    "    * Must \"prune\" the tree to reduce potential overfitting\n",
    "    * scikit-learn: `sklearn.tree.DecisionTreeClassifier`\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "* Ensemble methods - learn multiple models an dcombine results, usually via majority vote or averaging\n",
    "* Set of decision trees, each learned from a different randomly sampled subset with replacement\n",
    "* Features to split on for each tree, randomly selected subset from original features\n",
    "* Prediction: average output probabilities\n",
    "* Increases diversity through random selection of training dataset and subset of features for each tree\n",
    "* Reduces variance through averaging\n",
    "* Each tree typically does not need to be pruned\n",
    "* More expensive to train and run\n",
    "* scikit-learn: `sklearn.ensemble.RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "#### K-means clustering\n",
    "\n",
    "* Iteratively separates data into K clusters, minimizing sum of distances to center of closest cluster\n",
    "    * Step 1 - assign each instance to closest cluster\n",
    "    * Step 2 - recompute each center from assigned instances\n",
    "* Guaranteed to converge to local optimum\n",
    "* Suffers from curse of dimensionality\n",
    "* scikit-learn: `sklearn.cluster.kmeans`\n",
    "* User must determine or provide number of clusters (K)\n",
    "* Error is determined using sum of squared errors (SSE)\n",
    "\n",
    "$$ SSE = \\sum_{j=1}^n\\sum_{i=1}^m\\left\\lVert x_i - c_j \\right\\rVert^2_2 $$\n",
    "\n",
    "* Where $ c_j $ is the $ j^{th} $ cluster centroid and $ x $ is the number of samples belonging to the $ j^{th} $ cluster\n",
    "* Calculate this once cluster structure has been stabilized\n",
    "\n",
    "* Elbow method - use the elbow point as a starting point to determine how many clusters you should use\n",
    "    * More clusters implies smaller within-cluster SSE\n",
    "    * The decline of SSE (y-axis) slows down after the optimum number of clusters (x-axis) (i.e. the elbow point)\n",
    "\n",
    "![elbow](images/elbow.png)\n",
    "\n",
    "* Remember that if each cluster reaches size = 1, SSE will be zero but it's pretty useless\n",
    "\n",
    "#### Hierarchical clustering\n",
    "\n",
    "##### Agglomorative or \"Bottom-up\" \n",
    "* Bottom-up approach\n",
    "* Each data point begins as its own cluster\n",
    "\n",
    "##### Divisive\n",
    "* Top-down approach\n",
    "* Start with all points in a single cluster\n",
    "\n",
    "* Nested clusters with hierarchy\n",
    "* User doesn't need to provide number of clusters but needs to find a place to cut the dendrogram\n",
    "\n",
    "![elbow](images/dendrogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: Validation Set\n",
    "\n",
    "* Model training - improve model by optimizing parameters or data\n",
    "* Model tuning - tweak hyperparameters, looking for overfitting or underfitting\n",
    "* Motivation - model training and tuning involve comparing performance for different model or data settings\n",
    "* Problem - when you use the test set for these comparisons, that effectively makes it part of a training set (model may learn the patterns from the test set during training)\n",
    "* Solution - split training data into two parts - training and validation set\n",
    "    * Use training set to train candidate models, etc.\n",
    "    * The validation set plays the role of the test set during debugging and tuning\n",
    "    * Save the test set for measuring generalization of your final model\n",
    "* Validation set\n",
    "    * Issue - Splitting the training data into training and validation sets may make it too small or unrepresentative\n",
    "    * Solution - Use the holdout method to get the test set, then use k-fold cross validation on the training set for debugging and tuning\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: Bias Variance Tradeoff\n",
    "\n",
    "* **Bias:** an error from flawed assumptions in the algorithm. High bias can cause an algorithm to miss important relationships between features and target outputs resulting in underfitting.\n",
    "    * Try new features\n",
    "    * Decrease the degree of regularization\n",
    "* **Variance:** an error from sensitivity to small variations in the training data. High variance can cause an algorithm to model random noise in the training set, resulting in overfitting.\n",
    "    * Increase training data\n",
    "    * Decrease number of features\n",
    "\n",
    "![bias-variance tradeoff](images/biasvar1.png)\n",
    "\n",
    "#### Using learning curves to evaluate the model\n",
    "\n",
    "* Motivation - detect if model is under- or overfitting, and impact of training data size the error\n",
    "* learning curves - plot training dataset and validation dataset error or accuracy against training set size\n",
    "* scikit-learn: `sklearn.learning_curve.learning_curve`\n",
    "    * Uses stratified k-fold cross-validation by default if output is binary or multiclass (preserves percentage of samples in each class)\n",
    "    * Note: `sklearn.model_selection.learning_curve` in v 0.18\n",
    "\n",
    "![bias-variance tradeoff](images/biasvar2.png)\n",
    "\n",
    "#### Learning Curves\n",
    "\n",
    "![learning curves](images/learningcurves.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Debugging: Error Analysis\n",
    "\n",
    "* Filter on failed predictions and manually look for patterns\n",
    "* This helps you pivot on target, key attributes, and failure type, and build histograms of error counts\n",
    "* For classification problems, you can just create a df where predicted class isn't true class\n",
    "\n",
    "` error_df = test_df[pred != test_df['target']] `\n",
    "\n",
    "* Residual analysis - for regression\n",
    "* Some common patterns:\n",
    "    * Data problems (e.g., many variants for the same word)\n",
    "    * Labeling errors (e.g., data mislabeled)\n",
    "    * Under/over-represented subclasses (e.g., too many examples of one type)\n",
    "    * Discriminating information is not captured in features (e.g., customer locations)\n",
    "* NOTE: It often helps to look at what the model is predicting **correctly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning: Regularization\n",
    "\n",
    "* **Motivation** - Overfitting often caused by overly-complex models capturing idiosyncrasies in training set\n",
    "* **Regularization** - adding penalty term / score for complexity to cost function\n",
    "\n",
    "$$ cost_{reg} = cost + \\frac{\\lambda}{2} penalty $$\n",
    "\n",
    "* Usually the weight of each variable is a representation of the importance of that variable, but remember variables need to all be on the same scale for this to be meaningful\n",
    "* **Idea** - Large weights correspond to higher complexity\n",
    "    * Regularize by penalizing large weights\n",
    "* Two standard types:\n",
    "    * **L1 Regularization**: Sum all absolute values of values in vector\n",
    "        * Penalty = $ \\left\\lVert \\vec{w} \\right\\rVert_1 = \\sum^m_{j=1} | w_j |$\n",
    "        * L1 regularization is useful as a feature selection approach since most weights shrink to zero (sparsity)\n",
    "    * **L2 Regularization**: Sum of the squares of all the feature weights\n",
    "        * Penalty = $ \\left\\lVert \\vec{w} \\right\\rVert_2^2 = \\sum^m_{j=1} w^2_j $\n",
    "* **NOTE**: Important to scale features first!\n",
    "* scikit-learn: Models that support regularization typically provide parameters for type and strength\n",
    "\n",
    "* Contour represents sum of squared errors, where black dot in middle is SSE minimized\n",
    "* Diamond shape on left and \"bullseye\" on right represent contour of penalty term \n",
    "* Red dots below represent sum of SSE\n",
    "* Note that $ w_2 $ goes to zero in L1 regularization, but L2 is never actually reduced to zero.  \n",
    "    * So we're left with just $ w_1 $ as a feature for L1 regularization, but with L2, the weight is just severely reduced\n",
    "* The larger the lambda, the larger the regularization for same model and same training data\n",
    "* Ridge regression model: Linear regression with L2 - `sklearn.linear_model.Ridge`\n",
    "* Lasso regression model: Liner regression with L1 - `sklearn.linear_model.Lasso`\n",
    "* Elastic Net regression model: Linear regression with both - `sklearn.linear_model.ElasticNet`\n",
    "    * Parameter $ C $ is inverse of the regularization strength - smaller $ C $ means stronger regularization\n",
    "\n",
    "![regularization](images/reg001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning: Hyperparameter Tuning\n",
    "\n",
    "* Questions to address when choosing the optimal model\n",
    "    * What learning rate, how many nodes per layer, how many layers should I use for a NN model?\n",
    "    * What is the minimum number of samples I should use at the leaf node in a decision tree or random forest model?\n",
    "    * What is the optimum $ C $ parameter for a SVM or logistic regression model?\n",
    "* **Hyperparameter** - estimator parameter that is **not** fitted to the data\n",
    "* Hyperparameters must be optimized separately\n",
    "* Techniques:\n",
    "    * Grid search - every combination of each set of hyperparameters' possible values (compute intensive) - `sklearn.model_selection.GridSearchCV`\n",
    "    * Random search - `sklearn.model_selection.RandomizedSearchCV`\n",
    "* Look into `scipy.stats.expon` for values like $ C $\n",
    "* Hyperparameter tuning really depends on what you've learned from the business problem and/or what you've learned from the algorithm\n",
    "    * Cumulative process - once you've acquired a lot of knowledge about the problem and the algorithm, you'll have a good idea about how to setup the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a19cbb128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import expon\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "# Calculate a few first moments:\n",
    "\n",
    "mean, var, skew, kurt = expon.stats(moments='mvsk')\n",
    "# Display the probability density function (pdf):\n",
    "\n",
    "x = np.linspace(expon.ppf(0.01), expon.ppf(0.99), 100)\n",
    "ax.plot(x, expon.pdf(x), 'r-', lw=5, alpha=0.6, label='expon pdf')\n",
    "# Alternatively, the distribution object can be called (as a function) to fix the shape, location and scale parameters. This returns a “frozen” RV object holding the given parameters fixed.\n",
    "\n",
    "# Freeze the distribution and display the frozen pdf:\n",
    "\n",
    "rv = expon()\n",
    "ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "\n",
    "* Training set too small?\n",
    "    * Sample and label more data if possible\n",
    "* Training set biased against or missing some important scenarios?\n",
    "    * Sample and label more data for those scenarios if possible\n",
    "* Can't easily sample or label more?\n",
    "    * Consider creating synthetic data (duplication or techniques like [SMOTE - Synthetic Minority Over-sampling Technique](https://jair.org/index.php/jair/article/view/10302))\n",
    "    * Undersampling used with a group of observations that are abandoned\n",
    "    * Oversampling used with a group of observations that are really small\n",
    "    * Sometimes we do undersampling, sometimes we do oversampling, sometimes we do both on the same dataset\n",
    "* IMPORTANT - Training data doesn't need to be exactly representative (\"Whatever works\"), but your test set does\n",
    "* e.g., Fraud detection - can just duplicate data points that are representative of minority class (fraud in this case)\n",
    "\n",
    "* Feature Set Tuning\n",
    "    * Add features that help capture pattern for classes of errors\n",
    "        * **Example** - Many customers who are assigned to the women's dresses sale campaign are male -> add P(female|name) feature\n",
    "    * Try different transformations of the same feature\n",
    "        * **Example** - Perform multiple transformations of the values of a feature (as in earlier example, square, cube, sqrt, log, etc.)\n",
    "    * Apply dimensionality reduction to reduce impact of weak features\n",
    "* Interaction features - a * b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning: Feature Extraction\n",
    "\n",
    "* Too many features can lead to overfitting\n",
    "* Feature extraction maps data into smaller feature space that captures the bulk of the information in the data\n",
    "    * AKA data compression\n",
    "* Motivation\n",
    "    * Improves computational efficiency\n",
    "    * Reduces curse of dimensionality\n",
    "* Techniques\n",
    "    * Principal components analysis (PCA)\n",
    "        * Unsupervised lienar approach for feature extraction\n",
    "        * Finds patters based on correlations b/t features\n",
    "        * Constructs principal components: orthogonal axes in directions of maximum variance\n",
    "        * `sklearn.decomposition.PCA`\n",
    "        * Create this object with `n_components` parameter, then call its `fit_transform()` passing in the training set, then call a fit method on the resulting object (`X_train_pca`)\n",
    "        * Also have \"Kernel PCA\" in scikit-learn for nonlinear relationships - think of a feature like \"distance from center\" for the image below\n",
    "    * Linear discriminant analysis (LDA)\n",
    "    * Kernel versions of these for fundamentally non-linear data\n",
    "    \n",
    "![Kernel PCA would be good here](images/pca001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning: Feature Selection\n",
    "\n",
    "* Filter Methods\n",
    "    * Generally used in the pre-processing step to filter out features using statistical tests\n",
    "    * Measures the correlation b/t each independent and dependent variable before training\n",
    "    * Computationally less expensive\n",
    "    * Includes\n",
    "        * Pearson correlation\n",
    "        * Chi-squared test\n",
    "        * ANOVA test\n",
    "        * Information gain\n",
    "* Wrapper Methods\n",
    "    * Generally used during training to measure performance of each feature\n",
    "    * Computationally very expensive\n",
    "    * Includes\n",
    "        * Genetic algorithms\n",
    "        * Backward feature elimination algorithms\n",
    "        * Sequential feature selection algorithms\n",
    "* Embedded Methods\n",
    "    * Does feature selection as part of the algorithm itself\n",
    "    * Computationally less expensive than wrapper methods\n",
    "    * Example:\n",
    "        * Lasso regression\n",
    "        * Random Forest - great way to come up with a baseline, as well as determine which features are most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning: Bagging/Boosting\n",
    "\n",
    "#### Bagging\n",
    "\n",
    "* Approaches to semi-automate feature extraction and feature selection\n",
    "* Bagging - generate a group of weak learners that when combined together generate higher accuracy\n",
    "* Create a $ x $ datasets of size $ m $ by randomly sampling original dataset with replacement (duplicates allowed)\n",
    "* Train weak learners (decision stumps, logistic regression) on the new datasets to generate predictions\n",
    "* Choose the output by combining the individual predictions or voting\n",
    "* High variance, low bias?\n",
    "* Use bagging:\n",
    "    * Training many models on random subsets of the data and average/vote on the output\n",
    "    * Reduces variance\n",
    "    * Keeps bias the same\n",
    "    * sklearn:\n",
    "        * `sklearn.ensemble.BaggingClassifier`\n",
    "        * `sklearn.ensemble.BaggingRegressor`\n",
    "\n",
    "#### Boosting\n",
    "\n",
    "* Assign strengths to each weak learner\n",
    "* Iteratively train learners using misclassified examples\n",
    "* Model has a **high bias** and accepts weights on **individual samples**?\n",
    "* Use boosting:\n",
    "    * Training a sequence of samples to get a strong model\n",
    "        * Often times wins on datasets like most Kaggle competitions\n",
    "        * sklearn:\n",
    "            * `sklearn.ensemble.AdaBoostClassifier`\n",
    "            * `sklearn.ensemble.AdaBoostRegressor`\n",
    "            * `sklearn.ensemble.GradientBoostingClassifier`\n",
    "        * **XGBoost** library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
