{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing:  Encoding Categorical Variables\n",
    "\n",
    "\n",
    "* Pandas supports a data type of \"category\" but still ML models need numeric inputs\n",
    "* Ordinal - keep in mind relative magnitude (e.g., S, M, L, XL)\n",
    "* Can use Pandas `map()` function to map a dict of text keys to numerical values - keys are values in the column\n",
    "* Can use `sklearn.preprocessing.LabelEncoder()` for labels ONLY with binary variables\n",
    "    * this is the WRONG solution when there is no relationship between categories with more than two categories (OK for binary data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Encoding Nominal Variables\n",
    "\n",
    "* Can use Pandas `get_dummies()` function - automatically assigns column names as well\n",
    "* One-hot encoding - sometimes have to use `reshape(-1,1)` with the `OneHotEncoder()` from sklearn\n",
    "\n",
    "`from sklearn.preprocessing import OneHotEncoder`\n",
    "\n",
    "* Encoding with many classes - define a hierarchy structure\n",
    "    * Example - for a column with ZIP code, use regions -> states -> city as the hierarchy and choose a specific level to encode the ZIP code column\n",
    "* Try to group levels by similarity to reduce overall number of groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Handling Missing Values\n",
    "\n",
    "* Most ML models can't handle missing values\n",
    "* Pandas - check how many missing values for each column: `df. isnull.sum()`\n",
    "* Pandas - check how many missing values for each row: `df. isnull.sum(axis=1)`\n",
    "* Pandas - can use `dropna()` to drop rows with null values\n",
    "* Pandas - can use `dropna(axis=1)` to drop columns with null values\n",
    "    * `df.dropna(how='all')`\n",
    "    * `df.dropna(thresh=4)`\n",
    "    * `df.dropna(subset=['Fruits'])`\n",
    "* Risks of losing data - losing too much data leads to overfitting, wider confidence intervals, etc.\n",
    "* Too much data dropped can lead to bias\n",
    "* Risk of dropping columns - may lose information in features (underfitting)\n",
    "* Before dropping or imputing missing values, ask:\n",
    "    * What were the mechanisms that caused the missing values?\n",
    "    * Are these missing values missing at random?\n",
    "    * Are there rows or columns missing that you are not aware of?\n",
    "* If we believe missing values are random, can do imputation\n",
    "    * Mean or median for numeric variables\n",
    "    * Most frequent for categoricals\n",
    "    * Or any other estimated value\n",
    "* `from sklearn.preprocessing import Imputer`\n",
    "* `imputer = Imputer(strategy='mean')`\n",
    "* then call `imputer.fit()` and `imputer.transform()` on an object (like an array)\n",
    "* Advanced Methods for Imputing Missing values\n",
    "    * MICE (multiple imputation by chained equations) - `sklearn.impute.MICEImputer` (v 0.20)\n",
    "* Python (not sklearn) `fancyimpute` package\n",
    "    * KNN impute\n",
    "    * Soft impute\n",
    "    * MICE\n",
    "    * etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "* scikit-learn: `sklearn.feature_extraction`\n",
    "* often more art than science\n",
    "* rule of thumb - use intuition.  What information would a **human** use to predict this?\n",
    "* Try generating many features first, **then** apply dimensionality reduction if needed\n",
    "* Consider transformations of attributes - square a number, multiply two features together\n",
    "* Try not to overthink or include too much manual logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Filtering and Scaling\n",
    "\n",
    "* Images - remove channels from an image if color isn't important\n",
    "* Audio - remove frequencies from audio if power is less than a threshold\n",
    "* Many algorithms like kNN and gradient descent are sensitive to features being on different scales\n",
    "* Decision trees and random forests aren't usually  sensitive to features being on different scales\n",
    "* Scaling - want values between -1 and 1, or 0 and 1\n",
    "* Fit the scaler to training data only, then transform both train and validation data\n",
    "* Common choices for scaling in `sklearn`:\n",
    "    * Mean/variance  - z-score, subtract mean, divide by standard deviation - produces mean 0 and standard deviation 1 - `sklearn.preprocessing.StandardScaler`\n",
    "        * Advantages include many algorithms behave better with smaller values, and it keeps outlier information but reduces its impact\n",
    "    * MinMax scaling - minimum = 0, maximum = 1, then subtract min from each value and divide by max minus min - `sklearn.preprocessing.MinMaxScaler`\n",
    "        * Advantage: Robust to small standard deviations\n",
    "        * Difference b/t min and max is usually larger than standard deviation\n",
    "        * When SD is very small, mean/variance scaling isn't going to be robust, because we're dividing by a very small number\n",
    "    * Maxabs scaling - take maximum absolute value in dataset and divide everything by that\n",
    "        * Not centered, just scaled [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)\n",
    "    * Robust scaling - look at a feature in training data set and find median, 25th quantile and 75th quantile.  Then subtract median and divide by difference b/t 75th and 25th quantile.  This makes the feature robust to outliers, because outliers have little effect in calculating medians and quantiles\n",
    "* Scaling is for one column, normalizing is for one row\n",
    "* Normalizer - rescales $ x_j $ to unit norm based on \n",
    "    * L1 norm\n",
    "    * L2 norm\n",
    "    * Max norm\n",
    "* Normalizers used a lot in text analysis\n",
    "* `sklearn.preprocessing.Normalizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Transformation\n",
    "\n",
    "* Sometimes a polynomial is a better fit than a linear feature\n",
    "* `sklearn.preprocessing.PolynomialFeatures`\n",
    "    * Raises things to a power (all powers up to a power) as well as the interaction (a * b) between features\n",
    "* Beward of overfitting if the degree is too high\n",
    "* Consider non-polynomial transforms as well, like:\n",
    "    * Log transforms\n",
    "    * Sigmoid transforms\n",
    "* Risk of extrapolation beyond the range of data when using polynomial transformations\n",
    "* Radial Basis Function - transform the data through a center, represented below by $ c $\n",
    "* $ f(x) = f(\\left\\lVert x - c \\right\\rVert) $\n",
    "    * Widely used in SVM as a kernel and in Radial Basis Neural Networks (RBNNs)\n",
    "    * Gaussian RBF is the most common RBF used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering: Text-Based Features\n",
    "\n",
    "* Bag-of-words model\n",
    "    * Represent document as vector of numbers, one for each word (tokenize, count and normalize)\n",
    "* NOTE\n",
    "    * Sparse matrix implementation is typically used, ignores relative position of words\n",
    "    * Can be extended to bag of n-grams of words or characters\n",
    "* Count vectorizer\n",
    "    * Per-word value is count (also called term frequency)\n",
    "        * NOTE: Includes lowercasing and tokenization on white space and punctuation\n",
    "        * scikit-learn: `sklearn.feature_extraction.text.CountVectorizer`\n",
    "* `TfidfVectorizer` - Term Frequency times Inverse Document Frequency\n",
    "* Per-word value is *downweighted* for terms common across documents (e.g., \"the\")\n",
    "* `sklearn.feature_extraction.text.TfidfVectorizer`\n",
    "* A more efficient way of mapping words and counting frequencies is by using `HashingVectorizer` - stateless mapper from text to term index\n",
    "* `sklearn.feature_extraction.text.HashingVectorizer`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
